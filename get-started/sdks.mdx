---
title: Client SDKs

icon: terminal

description: We provide client libraries in a number of popular languages that make it easier to work with the PremAI API.
---

<Note>
We give you the option to use the [Prem AI API](/api-reference/introduction), the PremAI SDK, or the OpenAI SDK.

For API usage documentation, refer to the [API Reference](/api-reference/introduction).
</Note>

## Create an API Key ðŸ”‘

Click the **API Key** button on the sidebar. Then click the **+ Create API Key** button.

Afterwards, copy the API key and save it in a secure location.

<img
  src="https://static.premai.io/prem-saas-docs/get-started/quickstart-guide/create-api-key.gif"
  alt="Create an API Key"
/>

## Use with PremAI SDK

### Install the PremAI SDK
<CodeGroup>
```bash javascript/typescript
npm install premai
```
```bash python
pip install premai
```
</CodeGroup>

### List Models
<CodeGroup>
```javascript javascript/typescript
import PremAI from 'premai';

const client = new PremAI({
  apiKey: process.env['PREMAI_API_KEY'], // This is the default and can be omitted
});

const response = await client.models.list();

console.log(response.data);
```
```python python
import os
from premai import PremAI

client = PremAI(
    api_key=os.environ.get("PREMAI_API_KEY"),  # This is the default and can be omitted
)
response = client.models.list()
print(response.data)
```
</CodeGroup>

### Check Model Status
This is useful to check if a model is running or not. If it is not running, you can load it up using the `load` method.

<Note>
The model name can be replaced with the name of your fine-tuned models as well.
</Note>
<CodeGroup>
```javascript javascript/typescript
import PremAI from 'premai';

const client = new PremAI({
  apiKey: process.env['PREMAI_API_KEY'] // This is the default and can be omitted
});

const response = await client.models.check_status(model="llama3.2-3b");

console.log(response);
```
```python python
import os
from premai import PremAI

client = PremAI(
    api_key=os.environ.get("PREMAI_API_KEY")  # This is the default and can be omitted
)
response = client.models.check_status(model="llama3.2-3b")
print(response)
```
</CodeGroup>


### Load Model
This is useful to load up a model for inference.

<Note>
The model name can be replaced with the name of your fine-tuned models as well.
</Note>
<CodeGroup>
```javascript javascript/typescript
import PremAI from 'premai';

const client = new PremAI({
  apiKey: process.env['PREMAI_API_KEY'] // This is the default and can be omitted
});

const response = await client.models.load(model="llama3.2-3b"); // Or any other model you want to load up

const modelStatus = await client.models.check_status(model="llama3.2-3b");

console.log(modelStatus);
```
```python python
import os
from premai import PremAI

client = PremAI(
    api_key=os.environ.get("PREMAI_API_KEY")  # This is the default and can be omitted
)
response = client.models.load(
    model="llama3.2-3b", # Or any other model you want to load up
)
modelStatus = client.models.check_status(model="llama3.2-3b")
print(modelStatus.data)
```
</CodeGroup>

### Unload Model
This is useful to unload a model from the server.

<Note>
The model name can be replaced with the name of your fine-tuned models as well.
</Note>
<CodeGroup>
```javascript javascript/typescript
import PremAI from 'premai';

const client = new PremAI({
  apiKey: process.env['PREMAI_API_KEY'] // This is the default and can be omitted
});

const response = await client.models.unload(model="llama3.2-3b"); // Or any other model you want to load up

console.log(response);
```
```python python
import os
from premai import PremAI

client = PremAI(
    api_key=os.environ.get("PREMAI_API_KEY")  # This is the default and can be omitted
)
response = client.models.unload(
    model="llama3.2-3b", # Or any other model you want to load up
)
print(response)
```
</CodeGroup>

### Chat Completions
import PremChatCompletions from '/snippets/prem-chat-completions.mdx';

<PremChatCompletions/>

### Chat Completion with Streaming

<Warning>
Streaming is not supported for the PremAI SDK. Will be supported in the future.
</Warning>


## Use with OpenAI SDK

### Install the OpenAI SDK
<CodeGroup>
```bash javascript/typescript
npm install openai
```
```bash python
pip install openai
```
</CodeGroup>

### List Models
<CodeGroup>
```javascript javascript/typescript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: "https://studio.premai.io/api/v1/",
  apiKey: process.env['PREMAI_API_KEY'], // This is the default and can be omitted
});

const response = await client.models.list();

console.log(response.data);
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),  # This is the default and can be omitted
)
response = client.models.list()
print(response.data)
```
</CodeGroup>

### Chat Completions
<Note>
The model name can be replaced with the name of your fine-tuned models as well.
</Note>
<CodeGroup>
```javascript javascript/typescript
import OpenAI from "openai";

const client = new OpenAI({
    baseURL: "https://studio.premai.io/api/v1/",
    apiKey: process.env.PREMAI_API_KEY,
});

//Create a chat completion
const response = await client.chat.completions.create({
    model: "llama3.2-3b", //Or any other model you want to use
    messages: [{ role: "user", content: "Write a one-sentence bedtime story about a unicorn." }]
});

console.log(response.choices[0].message.content);
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create a chat completion
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
)

print(response.choices[0].message.content)
```
```python python (async)
import os
from openai import AsyncOpenAI

client = AsyncOpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create a chat completion
response = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
)

print(response.choices[0].message.content)
```
</CodeGroup>

### Chat Completion with Streaming
<CodeGroup>
```javascript javascript/typescript
import OpenAI from "openai";

const client = new OpenAI({
    baseURL: "https://studio.premai.io/api/v1/",
    apiKey: process.env.PREMAI_API_KEY,
});

//Create a chat completion
const response = await client.chat.completions.create({
    model: "llama3.2-3b", //Or any other model you want to use
    messages: [{ role: "user", content: "Write a one-sentence bedtime story about a unicorn." }],
    stream: true,
});

for await (const chunk of response) {
    process.stdout.write(chunk.choices[0].delta.content);
}
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create completion
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
    stream=True,
)

for chunk in response:
    print(chunk.choices[0].delta.content, end='', flush=True)
```
```python python (async)
import os
from openai import AsyncOpenAI

client = AsyncOpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create completion
response = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
    stream=True,
)

async for chunk in response:
    print(chunk.choices[0].delta.content, end='', flush=True)
```
</CodeGroup>
