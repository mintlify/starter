---
title: OpenTelemetry Integration Guide
sidebarTitle: Guide
---

import OpenInferenceMetadata from "/snippets/openinference-metadata.mdx";

OpenTelemetry is a standard protocol for tracing, and LangWatch is fully compatible with OpenTelemetry, you can use any OpenTelemetry compatible library to capture your LLM traces and send them to LangWatch.

This guide demonstrates the OpenTelemetry integration using Python, but the same principles apply to integration with OpenTelemetry instrumentation in other languages.

import LLMsTxtProtip from "/snippets/llms-txt-protip.mdx";

<LLMsTxtProtip />

#### Prerequisites

- Obtain your `LANGWATCH_API_KEY` from the [LangWatch dashboard](https://app.langwatch.com/).

#### Installation

```bash
pip install opentelemetry
```

#### Configuration

Set up LangWatch as the OpenTelemetry exporter endpoint:

```python
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

# Set up OpenTelemetry trace provider with LangWatch as the endpoint
tracer_provider = trace_sdk.TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(
        OTLPSpanExporter(
            endpoint="https://app.langwatch.ai/api/otel/v1/traces",
            headers={"Authorization": "Bearer " + os.environ["LANGWATCH_API_KEY"]},
        )
    )
)
# Optionally, you can also print the spans to the console.
tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
```

## Capturing LLM Traces

Currently, there are different open initiatives for LLM instrumentation libraries, here we show some examples on how to capture LLM traces with a couple of them.

<Tabs>
  <Tab title="OpenInference (Python)">
    <Tabs>
      <Tab title="OpenAI">
        Installation:

        ```bash
        pip install openinference-instrumentation-openai
        ```

        Then, instrument your OpenAI calls:

        ```python
        from openinference.instrumentation.openai import OpenAIInstrumentor

        OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your OpenAI calls in the LangWatch dashboard:

        ![OpenInference OpenAI Spans](/images/integration/opentelemetry/openinference-openai.png)

        <OpenInferenceMetadata />
      </Tab>
      <Tab title="AWS Bedrock">
        Installation:

        ```bash
        pip install openinference-instrumentation-bedrock
        ```

        Then, instrument your AWS calls:

        ```python
        from openinference.instrumentation.bedrock import BedrockInstrumentor

        BedrockInstrumentor().instrument()
        ```

        That's it! You can now see the traces for your AWS Bedrock calls in the LangWatch dashboard.

        <OpenInferenceMetadata />
      </Tab>
      <Tab title="DSPy">
        Installation:

        ```bash
        pip install openinference-instrumentation-dspy
        ```

        Then, instrument your DSPy calls:

        ```python
        from openinference.instrumentation.dspy import DSPyInstrumentor

        DSPyInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your DSPy calls in the LangWatch dashboard:

        ![OpenInference DSPy Spans](/images/integration/opentelemetry/openinference-dspy.png)

        <OpenInferenceMetadata />
      </Tab>
      <Tab title="Haystack">
        Installation:

        ```bash
        pip install openinference-instrumentation-haystack
        ```

        Then, instrument your Haystack calls:

        ```python
        from openinference.instrumentation.haystack import HaystackInstrumentor

        HaystackInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your Haystack calls in the LangWatch dashboard:

        ![OpenInference Haystack Spans](/images/integration/opentelemetry/openinference-haystack.png)

        <OpenInferenceMetadata />
      </Tab>
      <Tab title="LangChain">
        Installation:

        ```bash
        pip install openinference-instrumentation-langchain
        ```

        Then, instrument your LangChain calls:

        ```python
        from openinference.instrumentation.langchain import LangChainInstrumentor

        LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LangChain calls in the LangWatch dashboard:

        ![OpenInference LangChain Spans](/images/integration/opentelemetry/openinference-langchain.png)

        <OpenInferenceMetadata />
      </Tab>
      <Tab title="CrewAI">
        Installation:

        ```bash
        pip install openinference-instrumentation-crewai openinference-instrumentation-langchain
        ```

        Then, instrument your LangChain calls. CrewAI uses LangChain under the hood, so we instrument both:

        ```python
        from openinference.instrumentation.crewai import CrewAIInstrumentor
        from openinference.instrumentation.langchain import LangChainInstrumentor

        CrewAIInstrumentor().instrument(tracer_provider=tracer_provider)
        LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your CrewAI calls in the LangWatch dashboard.

        <OpenInferenceMetadata />
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="OpenLLMetry (Python)">
    <Tabs>
      <Tab title="OpenAI">
        Installation:

        ```bash
        pip install opentelemetry-instrumentation-openai
        ```

        Then, instrument your OpenAI calls:

        ```python
        from opentelemetry.instrumentation.openai import OpenAIInstrumentor

        OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your OpenAI calls in the LangWatch dashboard:

        ![OpenLLMetry OpenAI Spans](/images/integration/opentelemetry/openllmetry-openai.png)
      </Tab>
      <Tab title="Anthropic">
        Installation:

        ```bash
        pip install opentelemetry-instrumentation-anthropic
        ```

        Then, instrument your Anthropic calls:

        ```python
        from opentelemetry.instrumentation.anthropic import AnthropicInstrumentor

        AnthropicInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your Anthropic calls in the LangWatch dashboard:

        ![OpenLLMetry Anthropic Spans](/images/integration/opentelemetry/openllmetry-anthropic.png)
      </Tab>
      <Tab title="LangChain">
        Installation:

        ```bash
        pip install opentelemetry-instrumentation-langchain
        ```

        Then, instrument your LangChain calls:

        ```python
        from opentelemetry.instrumentation.langchain import LangchainInstrumentor

        LangchainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LangChain calls in the LangWatch dashboard:

        ![OpenLLMetry LangChain Spans](/images/integration/opentelemetry/openllmetry-langchain.png)
      </Tab>
      <Tab title="LlamaIndex">
        Installation:

        ```bash
        pip install opentelemetry-instrumentation-llamaindex
        ```

        Then, instrument your LlamaIndex calls:

        ```python
        from opentelemetry.instrumentation.llamaindex import LlamaIndexInstrumentor

        LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LlamaIndex calls in the LangWatch dashboard.
      </Tab>
    </Tabs>
  </Tab>
</Tabs>

