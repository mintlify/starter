# MigraÃ§Ã£o de Pipeline: Prefect 1.4 â†’ Prefect 3.0


## VisÃ£o Geral

Este guia irÃ¡ ajudÃ¡-lo a entender as principais diferenÃ§as e como migrar sua pipeline de forma segura.

### Principais MudanÃ§as

| Componente | Prefect 1.4 | Prefect 3.0 |
|------------|-------------|-------------|
| **Schedules** | Python code | YAML configuration |
| **Flows** | `@task` + `Flow()` | `@flow` + `@task` |
| **Deployments** | Python scripts | YAML files |
| **Agents** | Agents | Work Pools |
| **Configuration** | Python/JSON | YAML |

## PrÃ©-requisitos

Antes de comeÃ§ar a migraÃ§Ã£o, certifique-se de que vocÃª possui:

- âœ… Pipeline Prefect 1.4 funcionando corretamente
- âœ… Acesso completo ao cÃ³digo fonte da pipeline
- âœ… Conhecimento bÃ¡sico de YAML
- âœ… Acesso ao repositÃ³rio Git da pipeline

## Estrutura de Arquivos

### Estrutura Atual (Prefect 1.4)

```
minha-pipeline/
â”œâ”€â”€ flow.py              # DefiniÃ§Ã£o do flow e tasks
â”œâ”€â”€ schedules.py         # ConfiguraÃ§Ã£o de schedules
```

### Estrutura Nova (Prefect 3.0)

```
minha-pipeline/
â”œâ”€â”€ flow.py              # Flow com @flow decorator
â”œâ”€â”€ prefect.yaml         # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ Dockerfile           # Container
â””â”€â”€ pyproject.toml       # DependÃªncias Python
```

## Passo a Passo da MigraÃ§Ã£o

> **ðŸ’¡ Dica para UsuÃ¡rios de IDEs com LLM**: Se vocÃª usa Cursor ou outra IDE com LLM integrada, pode usar o seguinte prompt para automatizar a migraÃ§Ã£o:
>
> ```
> Quero migrar um flow do Prefect 1.4 para o Prefect 3.0. Para isso, siga as etapas abaixo, usando apenas os arquivos cujos caminhos fornecerei:
> 
> ### Arquivos:
> - Schedule original (Prefect 1.4): [CAMINHO_DO_SCHEDULE_ANTIGO]
> - YAML de referÃªncia (Prefect 3.0): [CAMINHO_YAML_REFERENCIA]
> - YAML a ser criado (Prefect 3.0): [CAMINHO_YAML_NOVO]
> 
> - Flow base (bem escrito): [CAMINHO_FLOW_REFERENCIA]
> - Flow a ser ajustado: [CAMINHO_FLOW_A_MODIFICAR]
> 
> ---
> 
> ### Etapas:
> 
> 1. **Migrar o Schedule:**
>    - Pegue as informaÃ§Ãµes do `schedule` do Prefect 1.4 (arquivo original).
>    - Converta para formato compatÃ­vel com o Prefect 3.0.
>    - Aplique o estilo e formataÃ§Ã£o usados no YAML de referÃªncia.
>    - Gere um novo arquivo YAML no local especificado.
> 
> 2. **Adaptar o Flow:**
>    - Use o flow de referÃªncia como modelo de estilo e boas prÃ¡ticas.
>    - Modifique o flow de destino para fazer a mesma coisa (ou o equivalente), mas adaptando os nomes de variÃ¡veis, funÃ§Ãµes e fluxo de execuÃ§Ã£o conforme o nome da pasta onde o flow de destino estÃ¡ localizado (use o nome da pasta final do caminho como referÃªncia).
>    - Garanta compatibilidade com Prefect 3.0.
> 
> 3. **Output:**
>    - Gere o conteÃºdo final do YAML convertido e do novo flow modificado.
>    - Comente resumidamente o que foi alterado em relaÃ§Ã£o aos originais.
> ```

### 1. Gerar Estrutura Base com CookieCutter

Use nosso template do cookiecutter para gerar rapidamente a estrutura de diretÃ³rios e arquivos necessÃ¡rios para uma nova pipeline Prefect. Para criar uma nova pipeline, instale [uv](https://docs.astral.sh/uv/) e rode:

```bash
uvx cookiecutter templates --output-dir=pipelines
```

VocÃª serÃ¡ solicitado a informar valores como secretaria e pipeline, que serÃ£o utilizados para preencher os nomes dos diretÃ³rios, arquivos e variÃ¡veis nos templates. O template gerado jÃ¡ segue o padrÃ£o de nomenclatura definido anteriormente.

O template irÃ¡ gerar automaticamente:
- `flow.py` - Estrutura base do flow
- `prefect.yaml` - ConfiguraÃ§Ã£o do deployment
- `Dockerfile` - Container para execuÃ§Ã£o
- `pyproject.toml` - DependÃªncias do projeto
- `.dockerignore` - Arquivos ignorados no build

### 2. AnÃ¡lise da Pipeline Atual

Antes de migrar, analise sua pipeline atual:

#### 2.1 Analisar ConfiguraÃ§Ãµes Atuais

Identifique as configuraÃ§Ãµes da sua pipeline de dump de banco:

- **Schedules**: FrequÃªncia, horÃ¡rio e timezone de execuÃ§Ã£o
- **ParÃ¢metros**: ConfiguraÃ§Ãµes especÃ­ficas do banco e tabelas
- **DependÃªncias**: Imports e bibliotecas utilizadas

### 3. MigraÃ§Ã£o do Schedule

#### 3.1 Analisar o Schedule Atual

Examine seu arquivo `schedules.py` atual:

```python
# Exemplo: schedules.py (Prefect 1.4)
from prefect.schedules import Schedule
from datetime import timedelta, datetime
import pytz
from prefeitura_rio.pipelines_utils.prefect import generate_dump_db_schedules

# ConfiguraÃ§Ã£o do schedule
sme_clocks = generate_dump_db_schedules(
    interval=timedelta(days=1),
    start_date=datetime(2022, 1, 1, 21, 0, tzinfo=pytz.timezone("America/Sao_Paulo")),
    table_parameters=sme_core_sso_queries,
)

schedule = Schedule(clocks=untuple(sme_clocks))
```

#### 3.2 Converter para YAML

Crie a seÃ§Ã£o de schedules no `prefect.yaml`:

```yaml
# prefect.yaml
name: minha-pipeline
prefect-version: 3.4.3

deployments:
  - name: minha-pipeline--prod
    entrypoint: flow.py:main_flow
    work_pool:
      name: default-agent-pool
      work_queue_name: default
    schedules:
      - interval: 86400  # 24 horas em segundos
        anchor_date: "2022-01-01T21:00:00"
        timezone: America/Sao_Paulo
        slug: daily-execution
        parameters:
          table_id: minha_tabela
          execute_query: |
            SELECT * FROM minha_tabela 
            WHERE data_atualizacao >= CURRENT_DATE - 1;
```

#### 3.3 Mapeamento de ConfiguraÃ§Ãµes

| Prefect 1.4 | Prefect 3.0 | Exemplo |
|-------------|-------------|---------|
| `timedelta(days=1)` | `interval: 86400` | 86400 segundos = 1 dia |
| `timedelta(hours=6)` | `interval: 21600` | 21600 segundos = 6 horas |
| `start_date` | `anchor_date` | "2022-01-01T21:00:00" |
| `timezone` | `timezone` | "America/Sao_Paulo" |
| `table_parameters` | `parameters` | ParÃ¢metros especÃ­ficos |

### 4. MigraÃ§Ã£o do Flow

#### 4.1 Identificar o Template

Para pipelines de dump de banco, use o template especÃ­fico:

```python
from iplanrio.pipelines_templates.dump_db.tasks import (
    dump_upload_batch_task,
    format_partitioned_query_task,
)
```

#### 4.2 Adaptar Imports

```python
# Antes (Prefect 1.4)
from prefect import task, Flow
from prefect.schedules import Schedule
from prefeitura_rio.pipelines_utils.prefect import generate_dump_db_schedules

# Depois (Prefect 3.0)
from prefect import task, flow
from iplanrio.pipelines_templates.dump_db.tasks import (
    dump_upload_batch_task,
    format_partitioned_query_task,
)
```

#### 4.3 Adaptar Decorators

```python
# Antes (Prefect 1.4)
@task
def minha_task():
    return "resultado"

def main_flow():
    result = minha_task()
    return result

# Criar o flow
with Flow("minha-pipeline") as flow:
    main_flow()

# Depois (Prefect 3.0)
@task
def minha_task():
    return "resultado"

@flow(log_prints=True, name="minha-pipeline")
def main_flow():
    result = minha_task()
    return result
```

#### 4.4 Adaptar ParÃ¢metros

```python
# Antes (Prefect 1.4)
def main_flow(param1, param2="default"):
    # lÃ³gica da pipeline
    pass

# Depois (Prefect 3.0)
@flow(log_prints=True)
def main_flow(
    param1: str,
    param2: str = "default",
    param3: Optional[str] = None,
):
    # lÃ³gica da pipeline
    pass
```

### 5. ConfiguraÃ§Ã£o do prefect.yaml

#### 5.1 Estrutura BÃ¡sica

```yaml
name: minha-pipeline
prefect-version: 3.4.3

build:
  - prefect.deployments.steps.run_shell_script:
      id: get-commit-hash
      script: git rev-parse --short HEAD
      stream_output: false
  - prefect_docker.deployments.steps.build_docker_image:
      id: build-image
      requires: prefect-docker>=0.6.5
      image_name: seu-registro/imagem
      tag: "minha-pipeline-{{ get-commit-hash.stdout }}"
      dockerfile: Dockerfile

deployments:
  - name: minha-pipeline--staging
    version: "{{ build-image.tag }}"
    entrypoint: flow.py:main_flow
    work_pool:
      name: default-agent-pool
      work_queue_name: default
      job_variables:
        image: "{{ build-image.image_name }}:{{ build-image.tag }}"
        command: python -m prefect flow-run execute
```

#### 5.2 ConfiguraÃ§Ã£o de Schedules

```yaml
deployments:
  - name: minha-pipeline--prod
    version: "{{ build-image.tag }}"
    entrypoint: flow.py:main_flow
    work_pool:
      name: default-agent-pool
      work_queue_name: default
      job_variables:
        image: "{{ build-image.image_name }}:{{ build-image.tag }}"
        command: python -m prefect flow-run execute
    schedules:
      - interval: 86400  # 24 horas
        anchor_date: "2022-01-01T21:00:00"
        timezone: America/Sao_Paulo
        slug: daily-execution
        parameters:
          param1: valor1
          param2: valor2
```

### 6. ConfiguraÃ§Ã£o de Work Pool

Este repositÃ³rio utiliza dois work pools principais para execuÃ§Ã£o dos deployments Prefect:

- **default-pool**: Destinado Ã  execuÃ§Ã£o geral de pipelines, incluindo fluxos que nÃ£o possuem requisitos especiais de rede ou infraestrutura. Ã‰ o pool padrÃ£o para a maioria dos deployments.

- **datario-pool**: Utilizado para pipelines que acessam bancos de dados ou sistemas internos da IplanRio, especialmente aqueles que exigem conexÃ£o via VPN. Esse pool garante que os jobs sejam executados em ambientes com acesso seguro e autorizado aos recursos internos.

Para pipelines de dump de banco de dados, use o **datario-pool**:

```yaml
work_pool:
  name: datario-pool
  work_queue_name: default
  job_variables:
    image: "{{ build-image.image_name }}:{{ build-image.tag }}"
    command: python -m prefect flow-run execute
```

### 7. MigraÃ§Ã£o de Secrets para Infisical

Para pipelines de dump de banco de dados, Ã© necessÃ¡rio configurar as credenciais de acesso no Infisical. Acesse [infisical.iplan.dados.rio](https://infisical.iplan.dados.rio) e configure os secrets nos projetos:

- **prefect-jobs** (produÃ§Ã£o)
- **prefect-jobs-staging** (staging)

#### 7.1 Estrutura de Pastas

Crie uma pasta com o nome da pipeline seguindo o seguinte padrÃ£o. Exemplo para a pipeline db-gestao-escolar:

```
Pipeline: db-gestao-escolar
â”œâ”€â”€ Pasta: db-gestao-escolar
    â”œâ”€â”€ DB_GESTAO_ESCOLAR__DB_USERNAME
    â”œâ”€â”€ DB_GESTAO_ESCOLAR__DB_PASSWORD
    â””â”€â”€ Outras variÃ¡veis especÃ­ficas
```

#### 7.2 PadrÃ£o de Nomenclatura

Para pipelines de ingestÃ£o de banco de dados, use sempre o padrÃ£o:

- `{PIPELINE_NAME_UPPER}__DB_USERNAME`
- `{PIPELINE_NAME_UPPER}__DB_PASSWORD`


#### 7.3 ConfiguraÃ§Ã£o no Flow

No seu flow, referencie as variÃ¡veis usando o caminho do Infisical:

```python
@flow(log_prints=True)
def dump_database_flow(
    # ... outros parÃ¢metros ...
    infisical_secret_path: str = "/db-gestao-escolar",
):
    # O Prefect irÃ¡ buscar automaticamente as variÃ¡veis
    # DB_GESTAO_ESCOLAR__DB_USERNAME e DB_GESTAO_ESCOLAR__DB_PASSWORD
    # no caminho /db-gestao-escolar do Infisical
```

## Exemplos PrÃ¡ticos

## Exemplo Completo

### Flow (flow.py)

```python
from prefect import flow, task
from iplanrio.pipelines_templates.dump_db.tasks import (
    dump_upload_batch_task,
    format_partitioned_query_task,
)

@flow(log_prints=True)
def dump_database_flow(
    db_database: str = "meu_banco",
    db_host: str = "localhost",
    db_port: str = "1433",
    db_type: str = "sql_server",
    execute_query: str = "SELECT * FROM minha_tabela",
    dataset_id: str = "meu_dataset",
    table_id: str = "minha_tabela",
    infisical_secret_path: str = "/secrets/database",
    dump_mode: str = "overwrite",
    batch_size: int = 50000,
    biglake_table: bool = True,
):
    print(f"Iniciando dump da tabela {table_id}")
    
    # Executar o dump
    result = dump_upload_batch_task(
        db_database=db_database,
        db_host=db_host,
        db_port=db_port,
        db_type=db_type,
        execute_query=execute_query,
        dataset_id=dataset_id,
        table_id=table_id,
        infisical_secret_path=infisical_secret_path,
        dump_mode=dump_mode,
        batch_size=batch_size,
        biglake_table=biglake_table,
    )
    
    print(f"Dump concluÃ­do: {result}")
    return result
```

### ConfiguraÃ§Ã£o (prefect.yaml)

```yaml
name: dump-database-pipeline
prefect-version: 3.4.3

build:
  - prefect.deployments.steps.run_shell_script:
      id: get-commit-hash
      script: git rev-parse --short HEAD
      stream_output: false
  - prefect_docker.deployments.steps.build_docker_image:
      id: build-image
      requires: prefect-docker>=0.6.5
      image_name: seu-registro/imagem
      tag: "dump-database-{{ get-commit-hash.stdout }}"
      dockerfile: Dockerfile

deployments:
  - name: dump-database--prod
    version: "{{ build-image.tag }}"
    entrypoint: flow.py:dump_database_flow
    work_pool:
      name: datario-pool
      work_queue_name: default
      job_variables:
        image: "{{ build-image.image_name }}:{{ build-image.tag }}"
        command: python -m prefect flow-run execute
    schedules:
      - interval: 86400
        anchor_date: "2022-01-01T21:00:00"
        timezone: America/Sao_Paulo
        slug: daily-dump
        parameters:
          db_database: "meu_banco"
          table_id: "minha_tabela"
          execute_query: "SELECT * FROM minha_tabela WHERE data_atualizacao >= CURRENT_DATE - 1"
```



 